# Databricks on Azure - Terraform Infrastructure

Complete Terraform infrastructure for deploying Databricks workspaces on Azure with medallion architecture (landing, bronze, silver, gold).

---

## ğŸ“‹ Quick Start

```bash
# 1. Initialize Terraform
terraform init

# 2. Plan deployment (review changes)
terraform plan -var-file=dev.tfvars

# 3. Deploy infrastructure
terraform apply -var-file=dev.tfvars -auto-approve

# 4. Destroy everything (when done)
terraform destroy -var-file=dev.tfvars -auto-approve
```

---

## ğŸ—ï¸ Two-Stage Deployment Architecture

**â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€**
The deployment uses a **two-stage architecture** to handle Azure and Databricks resource dependencies:

### Stage 1: Azure Infrastructure
Creates the foundation resources that Databricks needs to exist:
- Resource Group
- Virtual Network with delegated subnets
- Storage Accounts (ADLS Gen2 + ML)
- Networking (NSG, subnet associations)
- Monitoring (Log Analytics, Application Insights)
- Security (Key Vault)
- **Databricks Workspace** (takes 3-5 minutes to provision)

### Stage 2: Databricks-Native Resources
After the workspace exists, deploys Databricks-specific resources that require workspace authentication:
- Unity Catalog (catalogs, schemas, external locations)
- Compute (clusters, SQL warehouses)
- User Groups and RBAC permissions
- Service Principals
- Secret Scopes
- Cluster Policies

**Why Two Stages?**
- Databricks provider needs `workspace_url` which doesn't exist until Stage 1 completes
- Authentication to workspace required for Stage 2 resources
- Prevents chicken-and-egg dependency issues
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

---

## ğŸ“ Project Structure

```
terraform/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ providers.tf                 # Provider configurations (Azure, Databricks, Random)
â”œâ”€â”€ locals.tf                    # Environment config and feature flags
â”œâ”€â”€ variables.tf                 # Input variables with validation
â”œâ”€â”€ outputs.tf                   # Outputs (workspace URL, storage, catalog names)
â”œâ”€â”€ infrastructure.tf            # Stage 1: Azure networking, storage, workspace
â”œâ”€â”€ databricks.tf                # Stage 2: Clusters, SQL warehouses, user groups
â”œâ”€â”€ security.tf                  # Stage 2: Service principals, secret scopes
â”œâ”€â”€ unity_catalog.tf             # Stage 2: Unity Catalog resources (if enabled)
â”œâ”€â”€ dev.tfvars                   # Development environment overrides
â”œâ”€â”€ prod.tfvars                  # Production environment configuration
â”œâ”€â”€ credentials.auto.tfvars      # Azure Service Principal credentials (gitignored)
â”‚
â”œâ”€â”€ advanced_tuning.tf.disabled  # Optional: 5 specialized cluster configs
â”œâ”€â”€ streaming.tf.disabled        # Optional: Real-time streaming clusters
â”œâ”€â”€ performance_tuning.tf.disabled # Optional: 5S framework cluster policies
â”œâ”€â”€ apps.tf.disabled             # Optional: Databricks Apps infrastructure
â””â”€â”€ lakebase.tf.disabled         # Optional: Lakehouse Federation (Postgres)
```

---

## ğŸ›ï¸ Configuration

### Environment Variables

Set these in `credentials.auto.tfvars` (file is gitignored):

```hcl
# Azure Service Principal
client_id       = "your-client-id"
client_secret   = "your-client-secret"
tenant_id       = "your-tenant-id"
subscription_id = "your-subscription-id"

# Databricks Workspace (populated after Stage 1)
databricks_host = "https://adb-xxxxx.azuredatabricks.net"

# Optional: Postgres for Lakebase
postgres_host     = "railway-postgres.railway.app"
postgres_port     = "5432"
postgres_database = "railway"
postgres_user     = "postgres"
postgres_password = "your-password"
```

### Feature Flags (`locals.tf`)

Control what gets deployed:

```hcl
locals {
  environments = ["dev"]  # or ["dev", "prod"]

  # Unity Catalog (requires metastore)
  unity_catalog_enabled = false
  metastore_id          = ""  # Set if you have a metastore

  # Cost Control
  deploy_minimal_resources = true  # Disables expensive optional features
}
```

### Cost-Optimized Defaults

The configuration is pre-set for **cost optimization**:
- Only `dev` environment deploys by default
- Expensive features disabled (`.tf.disabled` files)
- Serverless SQL Warehouses (pay per query)
- Single-node clusters where possible
- Auto-termination enabled (10-20 minutes)

---

## ğŸ’° Cost Management

### Monthly Cost Estimates

**Minimal Deployment** (default):
- Storage (ADLS Gen2): ~$50/month
- Monitoring (Log Analytics, App Insights): ~$20/month
- Networking (VNet, NSG): ~$10/month
- Key Vault: ~$5/month
- **Compute (stopped):** $0
- **Total:** ~$85/month base infrastructure

**With Compute Running:**
- SQL Warehouse (2X-Small, Serverless): ~$0.40/hour
- All-Purpose Cluster (Standard_DS3_v2, single-node): ~$0.50/hour
- **4-hour demo session:** ~$2-3

**Full Deployment** (all features enabled):
- Add 9 specialized clusters: +$500-1000/month
- Add streaming clusters: +$200-400/month
- Add cluster policies and apps: +$100-200/month
- **Total:** ~$1,200-1,500/month

---

## ğŸš€ Deployment Workflows

### Development Environment Only

```bash
# Quick dev deployment
terraform apply -var-file=dev.tfvars -auto-approve

# Outputs workspace URL and storage account name
terraform output databricks_workspace_urls
```

### Enable Optional Features

```bash
# Re-enable disabled modules
mv advanced_tuning.tf.disabled advanced_tuning.tf
mv streaming.tf.disabled streaming.tf

# Deploy with additional features
terraform apply -var-file=dev.tfvars -auto-approve
```

---

## ğŸ” Authentication

### Azure CLI (Recommended for Local Development)

```bash
az login
az account set --subscription YOUR_SUBSCRIPTION_ID

# Databricks provider automatically uses Azure CLI tokens
```

---

## ğŸ“Š Deployed Resources

### Stage 1: Azure Infrastructure (22 resources)
- âœ… Resource Group, Databricks Workspace
- âœ… ADLS Gen2 with medallion architecture
- âœ… Virtual Network, Subnets, NSG
- âœ… Key Vault, Log Analytics, Application Insights

### Stage 2: Databricks Resources (10 resources)
- âœ… SQL Warehouse (Serverless)
- âœ… All-Purpose Cluster (single-node)
- âœ… User Groups with RBAC
- âœ… Service Principal, Secret Scope

---

## ğŸ§¹ Cleanup

```bash
terraform destroy -var-file=dev.tfvars -auto-approve
```

---

## ğŸ”§ Troubleshooting

### Workspace Access Denied

```bash
# Get your Azure AD Object ID
az ad signed-in-user show --query id -o tsv

# Assign Contributor role
az role assignment create \
  --assignee YOUR_OBJECT_ID \
  --role "Contributor" \
  --scope /subscriptions/SUB_ID/resourceGroups/ubereats-dev-rg/providers/Microsoft.Databricks/workspaces/ubereats-dev-workspace
```

---

**Deployed with â¤ï¸ for Semana Databricks 2.0**
